{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocking_hash as hash \n",
    "import blocking_ngram as ngram\n",
    "import blocking_structured_and_sort as ss\n",
    "import matchers as m\n",
    "import similarity as sim\n",
    "import cluster as c\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dblp_csv = '../CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_pairs_to_csv(similar_pairs, output_csv_file):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(output_csv_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        for pair in similar_pairs:\n",
    "            writer.writerow(pair)\n",
    "\n",
    "def evaluate_similarity(baseline, comparison):\n",
    "    baseline_set, comparison_set = set(baseline), set(comparison)\n",
    "\n",
    "    tp = len(baseline_set.intersection(comparison_set))\n",
    "    fp = len(comparison_set - baseline_set)\n",
    "    fn = len(baseline_set - comparison_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    result = {'precision': precision, 'recall': recall, 'f_measure': f_measure}\n",
    "\n",
    "    return str(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Baselines we make a row wise comparison where we compare respective columns\n",
    "The idea is also to not use id, because both datasets have different ids even for corresponding enteties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines 0.7, 0.85 \n",
    "\n",
    "dblp['year'] = dblp['year'].astype(str)\n",
    "acm['year'] = acm['year'].astype(str)\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_jac = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.jaccard_similarity)\n",
    "similar_pairs_to_csv(base_7_jac,'baselines/base_7_jac.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_jac = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.jaccard_similarity)\n",
    "similar_pairs_to_csv(base_85_jac,'baselines/base_85_jac.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_n = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.n_gram_similarity)\n",
    "similar_pairs_to_csv(base_7_n,'baselines/base_7_n.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_n = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.n_gram_similarity)\n",
    "similar_pairs_to_csv(base_85_n,'baselines/base_85_n.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_7_lev = m.apply_similarity_baseline(dblp, acm, 0.7, selected_columns, sim.levensthein_distance)\n",
    "similar_pairs_to_csv(base_7_lev,'baselines/base_7_lev.csv')\n",
    "\n",
    "selected_columns = ['author_names','paper_title', 'year', 'publication_venue']\n",
    "base_85_lev = m.apply_similarity_baseline(dblp, acm, 0.85, selected_columns, sim.levensthein_distance)\n",
    "similar_pairs_to_csv(base_85_lev,'baselines/base_85_lev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the baselines take a time to compute so to not compute it again save csv and transform to list pairs\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "base_7_jac = reconstructed_pairs('../baselines/base_7_jac.csv')\n",
    "base_85_jac = reconstructed_pairs('../baselines/base_85_jac.csv')\n",
    "\n",
    "base_7_n = reconstructed_pairs('../baselines/base_7_n.csv')\n",
    "base_85_n = reconstructed_pairs('../baselines/base_85_n.csv')\n",
    "\n",
    "base_7_lev = reconstructed_pairs('../baselines/base_7_lev.csv')\n",
    "base_85_lev = reconstructed_pairs('../baselines/base_85_lev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 3.3457040786743164 seconds. Number of similar pairs: 1665\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = '../CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "\n",
    "hash_indices = ['hash_value']\n",
    "ngram_indices = ['ngram_values']\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_ap = m.apply_similarity_sorted(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 10.310678005218506 seconds. Number of similar pairs: 1235\n",
      "Processing time: 27.914609909057617 seconds. Number of similar pairs: 0\n",
      "Processing time: 9.807164907455444 seconds. Number of similar pairs: 1235\n",
      "Processing time: 33.01195693016052 seconds. Number of similar pairs: 0\n",
      "Processing time: 8.09684133529663 seconds. Number of similar pairs: 1235\n",
      "Processing time: 13.279062986373901 seconds. Number of similar pairs: 1099\n",
      "Processing time: 5.1999640464782715 seconds. Number of similar pairs: 1665\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = '../CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "\n",
    "hash_indices = ['hash_value']\n",
    "ngram_indices = ['ngram_values']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "dblp_n2 = ngram.initial_ngram(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.initial_ngram(acm, 2, selected_columns)\n",
    "initial_n2_ap_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n2 = ngram.n_gram_blocking(dblp, 2, selected_columns)\n",
    "acm_n2 = ngram.n_gram_blocking(acm, 2, selected_columns)\n",
    "n2_ap_07_jac = m.apply_similarity_blocks(dblp_n2, acm_n2, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_n3 = ngram.initial_ngram(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.initial_ngram(acm, 3, selected_columns)\n",
    "initial_n3_ap_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, ngram_indices)\n",
    "\n",
    "dblp_n3 = ngram.n_gram_blocking(dblp, 3, selected_columns)\n",
    "acm_n3 = ngram.n_gram_blocking(acm, 3, selected_columns)\n",
    "n3_ap_07_jac = m.apply_similarity_blocks(dblp_n3, acm_n3, threshold, sim.jaccard_similarity_ngrams, selected_columns)\n",
    "\n",
    "dblp_h = hash.initial_hash(dblp, selected_columns)\n",
    "acm_h = hash.initial_hash(acm, selected_columns)\n",
    "initial_h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "dblp_h = hash.hash_blocking(dblp, selected_columns)\n",
    "acm_h = hash.hash_blocking(acm, selected_columns)\n",
    "h_ap_jac = m.apply_similarity_blocks(dblp_h, acm_h, threshold, sim.jaccard_similarity, hash_indices)\n",
    "\n",
    "dblp_s = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "acm_s = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "sorted_ap = m.apply_similarity_sorted2(dblp_s, acm_s, threshold, sim.jaccard_similarity, selected_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.9433198380566802, 'recall': 0.7609405617243632, 'f_measure': 0.8423716558206797}\n",
      "{'precision': 0.9433198380566802, 'recall': 0.7609405617243632, 'f_measure': 0.8423716558206797}\n",
      "{'precision': 0.9433198380566802, 'recall': 0.7609405617243632, 'f_measure': 0.8423716558206797}\n",
      "{'precision': 0.9770220588235294, 'recall': 0.6943174395819726, 'f_measure': 0.8117602138220695}\n",
      "{'precision': 0.8994581577363034, 'recall': 0.9758327890267798, 'f_measure': 0.9360902255639099}\n"
     ]
    }
   ],
   "source": [
    "result_combined = (\n",
    "    evaluate_similarity(base_7_jac, initial_n2_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_n3_ap_07_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, initial_h_ap_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, h_ap_jac) + \"\\n\" +\n",
    "    evaluate_similarity(base_7_jac, sorted_ap)\n",
    ")\n",
    "\n",
    "print(result_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocks_to_df(blocks):\n",
    "    dfs = []\n",
    "    for block in blocks:\n",
    "        df_block = pd.DataFrame(block)\n",
    "        dfs.append(df_block)\n",
    "    return dfs\n",
    "\n",
    "def block_dfs(dataframes, blocking_function, *args):\n",
    "    blocks = {}\n",
    "    for df in dataframes:\n",
    "        block = blocking_function(df, *args)\n",
    "        blocks.update(block)\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def apply_similarity_sw(blocks1, blocks2, threshold, similarity_function, indices):\n",
    "    similar_pairs = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for (key1, block1), (key2, block2) in zip(blocks1.items(), blocks2.items()):\n",
    "        for elem1 in block1:\n",
    "            for elem2 in block2:\n",
    "                average_similarity = 0.0\n",
    "                if isinstance(block1, dict):\n",
    "                    print('block1 is a dictionary')\n",
    "                if isinstance(block2, dict):\n",
    "                    print('block2 is a dictionary')\n",
    "                \n",
    "                value_block1 = [elem1.get(i, '') for i in indices]\n",
    "                value_block2 = [elem2.get(i, '') for i in indices]\n",
    "\n",
    "                similarity = similarity_function(value_block1, value_block2)\n",
    "                average_similarity += similarity\n",
    "\n",
    "                if len(indices) > 1:\n",
    "                    average_similarity /= len(indices)\n",
    "\n",
    "                if average_similarity >= threshold:\n",
    "                    index_pair = (elem1['index'], elem2['index'])\n",
    "                    similar_pairs.append(index_pair)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing time: {elapsed_time} seconds. Number of similar pairs: {len(similar_pairs)}\")\n",
    "\n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 0.41853904724121094 seconds. Number of similar pairs: 2184\n"
     ]
    }
   ],
   "source": [
    "import blocking_hash as hash \n",
    "import blocking_ngram as ngram\n",
    "import blocking_structured_and_sort as ss\n",
    "import matchers as m\n",
    "import similarity as sim\n",
    "import csv\n",
    "\n",
    "\n",
    "# basically sorted neighboor hood blocking and then blocking the blocks again with ngram \n",
    "dblp_csv = '../CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "\n",
    "n = 2\n",
    "ngram_indices = ['ngram_values']\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "db = ss.block_by_year_and_publisher(dblp, year_block, labels)\n",
    "db = blocks_to_df(db)\n",
    "db = block_dfs(db, ngram.initial_ngram, n, selected_columns)\n",
    "\n",
    "ac = ss.block_by_year_and_publisher(acm, year_block, labels)\n",
    "ac = blocks_to_df(ac)\n",
    "ac = block_dfs(ac, ngram.initial_ngram, n, selected_columns)\n",
    "\n",
    "\n",
    "\n",
    "sorted_ap = m.apply_similarity_sorted_dictionary(db, ac, threshold, sim.jaccard_similarity_ngrams, ngram_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# return pairs in Format [1232, 2323]\n",
    "def read_matched_entities(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader) \n",
    "        matched_entities = [row for row in reader]\n",
    "    return matched_entities\n",
    "\n",
    "# print cluster based on the specific meh\n",
    "def print_clusters(clusters):\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f'Cluster {i + 1}: {cluster}')\n",
    "\n",
    "\n",
    "file_path = '../baselines/base_7_jac.csv'\n",
    "matched_entities = read_matched_entities(file_path)\n",
    "clusters = c.build_clusters(matched_entities)\n",
    "print_clusters(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other ER is too chaotic -> ask wednesday how to do it properly then do it here\n",
    "structure: baseline, respective matchers to compare, then compare and take the best ones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
