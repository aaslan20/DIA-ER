{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code für Blocking mit Intervall + Key (publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2153\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def block_by_year_ranges(df):\n",
    "    year_ranges = []\n",
    "    year_block = [1995, 1998, 2001, 2005]\n",
    "    labels = [\"1995-1997\", \"1998-2000\", \"2001-2004\"]\n",
    "\n",
    "    df[\"year_range\"] = pd.cut(df['year'], bins=year_block, labels=labels, right=False)\n",
    "    for label in labels:\n",
    "        intervals = df[df[\"year_range\"] == label].to_numpy()\n",
    "        year_ranges.append(intervals)\n",
    "\n",
    "    return year_ranges\n",
    "\n",
    "\n",
    "def block_by_year_ranges_key_dynamic(df, year_block, labels):\n",
    "    year_ranges = []\n",
    "    df[\"year_range\"] = pd.cut(df['year'], bins=year_block, labels=labels, right=False)\n",
    "    publishers = ['sigmod', 'vldb']  # List of publishers to separate data based on\n",
    "\n",
    "    for label in labels:\n",
    "        publisher_blocks = []\n",
    "        intervals = df[df[\"year_range\"] == label]\n",
    "\n",
    "        for publisher in publishers:\n",
    "            publisher_block = intervals[intervals[\"publication_venue\"].str.contains(publisher)]\n",
    "            if publisher_block.size > 0:\n",
    "                # publisher_block.iterrows() for  all columns?\n",
    "                publisher_blocks.append(publisher_block)\n",
    "\n",
    "        year_ranges.extend(publisher_blocks)\n",
    "\n",
    "    return year_ranges\n",
    "\n",
    "\n",
    "dblp_csv = '/Users/aliaslan/Downloads/DIA_Exercise/CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "dblp_blocks = block_by_year_ranges(dblp)\n",
    "print(len(dblp_blocks))\n",
    "acm_csv = '/Users/aliaslan/Downloads/DIA_Exercise/CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "acm_block = block_by_year_ranges(acm)\n",
    "\n",
    "year_block = [1995, 1998, 2001, 2005]\n",
    "labels = [\"1995-1997\", \"1998-2000\", \"2001-2004\"]\n",
    "dblp_blocks = block_by_year_ranges_key_dynamic(dblp, year_block, labels)\n",
    "length = len(dblp_blocks[0]) + len(dblp_blocks[1]) + len(dblp_blocks[2]) + len(dblp_blocks[3]) + len(dblp_blocks[4]) + len(dblp_blocks[5])\n",
    "print(length)\n",
    "print(len(dblp_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_by_year_ranges_key(df):\n",
    "    year_ranges = []\n",
    "    year_block = [1995, 1998, 2001, 2005]\n",
    "    labels = [\"1995-1997\", \"1998-2000\", \"2001-2004\"]\n",
    "\n",
    "    df[\"year_range\"] = pd.cut(df['year'], bins=year_block, labels=labels, right=False)\n",
    "    publishers = ['sigmod', 'vldb']  # List of publishers to separate data based on\n",
    "\n",
    "    for label in labels:\n",
    "        publisher_blocks = []\n",
    "        intervals = df[df[\"year_range\"] == label]\n",
    "\n",
    "        for publisher in publishers:\n",
    "            publisher_block = intervals[intervals[\"publication_venue\"].str.contains(publisher)]\n",
    "            if publisher_block.size > 0:\n",
    "                publisher_blocks.append(publisher_block)\n",
    "\n",
    "        year_ranges.extend(publisher_blocks)\n",
    "\n",
    "    return year_ranges\n",
    "    \n",
    "\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "dblp_blocks = block_by_year_ranges_key(dblp)\n",
    "length = len(dblp_blocks[0]) + len(dblp_blocks[1]) + len(dblp_blocks[2]) + len(dblp_blocks[3]) + len(dblp_blocks[4]) + len(dblp_blocks[5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocking with Hash - choose columns freely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_blocking(dataframe, blocking_columns):\n",
    "    dataframe['hash_key'] = dataframe[blocking_columns].astype(str).agg(''.join, axis=1)\n",
    "    dataframe['hash_value'] = dataframe['hash_key'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "    blocks = dataframe.groupby('hash_value').apply(lambda x: {'hash_value': x['hash_value'].iloc[0], 'index': x['index'].tolist()}).to_dict()\n",
    "    return blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocking by initials combined and the hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def initial_hash(df, columns):\n",
    "    df ['publication_venue'] = df['publication_venue'].apply(lambda x: 'sigmod' if 'sigmod' in x else 'vldb')\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        components_values = []\n",
    "\n",
    "        for column in columns:\n",
    "            if column == 'author_names':\n",
    "                author_initials = []\n",
    "                for name in row['author_names'].split():\n",
    "                    parts = name.split()\n",
    "                    if len(parts) > 1:\n",
    "                        author_initials.append(parts[0][0] + parts[-1][0])\n",
    "                    else:\n",
    "                        author_initials.append(name[0])\n",
    "                component_value = \"\".join(author_initials)\n",
    "            elif column == 'paper_title':\n",
    "                component_value = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "            elif column == 'publication_venue':\n",
    "                component_value = row['publication_venue']\n",
    "            elif column == 'year':\n",
    "                component_value = str(row[\"year\"])\n",
    "            \n",
    "\n",
    "            components_values.append(component_value)\n",
    "\n",
    "        blocking_key = ''.join(components_values)\n",
    "        blocking_key_hash = hashlib.md5(blocking_key.encode()).hexdigest()\n",
    "        block_data = {'ngram_values': blocking_key_hash, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocking with hash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram blocking - wähl die Columns freiwillig aus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def ngrams_tuple(s, n):\n",
    "    return [tuple(s[i:i+n]) for i in range(len(s)-n+1)]\n",
    "\n",
    "def hash_ngram_values(ngram_values):\n",
    "    return hashlib.sha256(str(hash(tuple(sorted(ngram_values.items())))).encode()).hexdigest()\n",
    "\n",
    "def n_gram_blocking(dataframe, columns, n):\n",
    "    # n-gram for each selected column \n",
    "    for column in columns:\n",
    "        key_name = f'ngram_key_{column}'\n",
    "        dataframe[key_name] = dataframe[column].apply(lambda x: ''.join(map(str, x)))\n",
    "\n",
    "        ngram_col_name = f'ngram_values_{column}'\n",
    "        dataframe[ngram_col_name] = dataframe[key_name].apply(lambda x: tuple(ngrams_tuple(x, n)))\n",
    "\n",
    "    # block them and each block consists of n-gram values and the respective index (id) \n",
    "    blocks = {}\n",
    "    for index, row in dataframe.iterrows():\n",
    "        ngram_values = {column: row[f'ngram_values_{column}'] for column in columns}\n",
    "        hashable_ngram_values = hash_ngram_values(ngram_values)\n",
    "        \n",
    "        if hashable_ngram_values in blocks:\n",
    "            blocks[hashable_ngram_values]['index'].add(row['index'])\n",
    "        else:\n",
    "            blocks[hashable_ngram_values] = {'index': {row['index']}, **ngram_values}\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# if you want to try out\n",
    "\"\"\"\"\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "n_value = 2\n",
    "result_blocks = n_gram_blocking(dblp, selected_columns, n_value)\n",
    "\n",
    "for key, block in result_blocks.items():\n",
    "    print(f\"Block key: {key}\")\n",
    "    print(f\"Block content: {block}\")\n",
    "    print(f\"Values for selected columns: {[block.get(i, '') for i in selected_columns]}\")\n",
    "    print(\"-----\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initials als combined string mit n-gram wobei Jahr komplett genommen wird und beim publisher_venue nur das Stichwort sigmod or vldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ngrams_string(s, n):\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "# created this afterwards so we dont need to use the other initial_..._ngram methods (didnt delete the others for know, \n",
    "# because I used them for matching can be changed after we decide our baseline)\n",
    "def initial_ngram(df, n, columns):\n",
    "    df ['publication_venue'] = df['publication_venue'].apply(lambda x: 'sigmod' if 'sigmod' in x else 'vldb')\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        components_values = []\n",
    "\n",
    "        for column in columns:\n",
    "            if column == 'author_names':\n",
    "                author_initials = []\n",
    "                for name in row['author_names'].split():\n",
    "                    parts = name.split()\n",
    "                    if len(parts) > 1:\n",
    "                        author_initials.append(parts[0][0] + parts[-1][0])\n",
    "                    else:\n",
    "                        author_initials.append(name[0])\n",
    "                component_value = \"\".join(author_initials)\n",
    "            elif column == 'paper_title':\n",
    "                component_value = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "            elif column == 'publication_venue':\n",
    "                component_value = row['publication_venue']\n",
    "            elif column == 'year':\n",
    "                component_value = str(row[\"year\"])\n",
    "        \n",
    "            components_values.append(component_value)\n",
    "\n",
    "        blocking_key = ''.join(components_values)\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def ngrams_string(s, n):\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "# ap = author + paper title\n",
    "\n",
    "def initial_ap_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "\n",
    "        blocking_key = combined_author_initials + paper_title_initials\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# author + publication venue\n",
    "def initial_apv_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "\n",
    "        venue_key = row[\"publication_venue\"] \n",
    "\n",
    "        publisher_keyword = \"sigmod\" if \"sigmod\" in venue_key else \"vldb\"\n",
    "        blocking_key = combined_author_initials + publisher_keyword\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# author + year\n",
    "def initial_ay_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "        \n",
    "        year_key = str(row[\"year\"])\n",
    "    \n",
    "        blocking_key = combined_author_initials + year_key\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def initial_apy_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "        year_key = str(row[\"year\"])\n",
    "\n",
    "        blocking_key = combined_author_initials + paper_title_initials + year_key\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "   \n",
    "\n",
    "def initial_apvy_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "    \n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "        year_key = str(row[\"year\"])\n",
    "        venue_key = row[\"publication_venue\"] \n",
    "\n",
    "     \n",
    "        publisher_keyword = \"sigmod\" if \"sigmod\" in venue_key else \"vldb\"\n",
    "        blocking_key = combined_author_initials + publisher_keyword + year_key\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def initial_appvy_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        author_initials = []\n",
    "        for name in row['author_names'].split():\n",
    "            parts = name.split()\n",
    "            if len(parts) > 1:\n",
    "                author_initials.append(parts[0][0] + parts[-1][0])\n",
    "            else:\n",
    "                author_initials.append(name[0])\n",
    "\n",
    "         \n",
    "        combined_author_initials = \"\".join(author_initials)\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "\n",
    "        year_key = str(row[\"year\"])\n",
    "        venue_key = row[\"publication_venue\"] \n",
    "\n",
    "        # Directly use \"sigmod\" or \"vldb\" in the blocking key\n",
    "        publisher_keyword = \"sigmod\" if \"sigmod\" in venue_key else \"vldb\"\n",
    "        blocking_key =  combined_author_initials + paper_title_initials + year_key  + publisher_keyword\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def initial_py_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "        year_key = str(row[\"year\"])\n",
    "\n",
    "        blocking_key = paper_title_initials + year_key\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# pp = paper_tite and publication venue\n",
    "def initial_ppv_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "               \n",
    "        venue_key = row[\"publication_venue\"] \n",
    "\n",
    "        # Directly use \"sigmod\" or \"vldb\" in the blocking key\n",
    "        publisher_keyword = \"sigmod\" if \"sigmod\" in venue_key else \"vldb\"\n",
    "        blocking_key = paper_title_initials + publisher_keyword\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n",
    "\n",
    "# like method before but with year\n",
    "def initial_ppvy_ngram(df, n):\n",
    "    blocks = {}\n",
    "    for index, row in df.iterrows():\n",
    "        paper_title_initials = \"\".join(word[0] for word in row['paper_title'].split())\n",
    "               \n",
    "        year_key = str(row[\"year\"])\n",
    "        venue_key = row[\"publication_venue\"] \n",
    "\n",
    "        # Directly use \"sigmod\" or \"vldb\" in the blocking key\n",
    "        publisher_keyword = \"sigmod\" if \"sigmod\" in venue_key else \"vldb\"\n",
    "        blocking_key = paper_title_initials + year_key  + publisher_keyword\n",
    "        blocking_key_ngrams = list(ngrams_string(blocking_key, n))\n",
    "        block_data = {'ngram_values': blocking_key_ngrams, 'index': [row['index']]}\n",
    "\n",
    "        if blocking_key in blocks:\n",
    "            unique_indices = list(set(blocks[blocking_key]['index'] + block_data['index']))\n",
    "            blocks[blocking_key]['index'] = unique_indices\n",
    "        else:\n",
    "            blocks[blocking_key] = block_data\n",
    "\n",
    "    return blocks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_similarity(df1, df2, threshold, selected_columns, similarity_function):\n",
    "    similar_pairs = []\n",
    "\n",
    "    for index1, row1 in df1.iterrows():\n",
    "        for index2, row2 in df2.iterrows():\n",
    "            average_similarity = 0.0\n",
    "\n",
    "            for col_df1, col_df2 in zip(selected_columns, selected_columns):\n",
    "                value_df1 = set(row1[col_df1].lower().split())\n",
    "                value_df2 = set(row2[col_df2].lower().split())\n",
    "\n",
    "                similarity = similarity_function(value_df1, value_df2)\n",
    "                average_similarity += similarity\n",
    "\n",
    "            if len(selected_columns) > 1:\n",
    "                average_similarity /= len(selected_columns)\n",
    "\n",
    "            if average_similarity >= threshold:\n",
    "                pair = (row1[selected_columns].to_dict(), row2[selected_columns].to_dict(), average_similarity)\n",
    "                similar_pairs.append(pair)\n",
    "\n",
    "    return similar_pairs\n",
    "\n",
    "\n",
    "\n",
    "def apply_similarity_blocks(blocks1, blocks2, threshold, similarity_function, indices):\n",
    "    similar_pairs = []\n",
    "\n",
    "    for key1, block1 in blocks1.items():\n",
    "        for key2, block2 in blocks2.items():\n",
    "            average_similarity = 0.0\n",
    "\n",
    "            value_block1 = [block1.get(i, '') for i in indices]\n",
    "            value_block2 = [block2.get(i, '') for i in indices]\n",
    "            similarity = similarity_function(value_block1, value_block2)\n",
    "\n",
    "            average_similarity += similarity\n",
    "\n",
    "            if len(indices) > 1:\n",
    "                average_similarity /= len(indices)\n",
    "\n",
    "            if average_similarity > threshold:\n",
    "                index_pairs = [(i, j) for i in block1['index'] for j in block2['index']]\n",
    "                similar_pairs.extend(index_pairs)\n",
    "                \n",
    "                \n",
    "    return similar_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intersection and union then divide it\n",
    "#def jaccard_similarity(set1, set2):\n",
    "#    intersection = len(set1 & set2)\n",
    "#    union = len(set1 | set2)\n",
    "#    return intersection / union if union > 0 else 0\n",
    "\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    if not isinstance(set1, set):\n",
    "        set1 = set(set1)\n",
    "    if not isinstance(set2, set):\n",
    "        set2 = set(set2)\n",
    "    \n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def levensthein_distance(set1, set2):\n",
    "    # make the subcriptable \n",
    "    list1, list2 = list(set1), list(set2)\n",
    "    m, n = len(list1), len(list2)\n",
    "\n",
    "    # create matrix\n",
    "    matrix = np.zeros((m + 1, n + 1))\n",
    "\n",
    "    # fill row with length of string \n",
    "    for i in range(1, m + 1):\n",
    "        matrix[i][0] = i\n",
    "\n",
    "    # fill column with lenth of string \n",
    "    for j in range(1, n + 1):\n",
    "        matrix[0][j] = j\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if list1[i - 1] ==  list2[j - 1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            \n",
    "            matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + cost)\n",
    "    similarity = 1 - (matrix[m][n] / (m + n))\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n_gram\n",
    "def get_n_grams(string, n):\n",
    "    characters = list(string.lower())\n",
    "    n_grams = [tuple(characters[i:i+n]) for i in range(len(characters) - (n-1))]\n",
    "    return n_grams\n",
    "\n",
    "# transform respective columns to n_gram\n",
    "def transform_columns_to_n_gram(df, n, columns):\n",
    "    for column in columns:\n",
    "        df[f'{column}'] = df[column].apply(lambda x: get_n_grams(str(x), n))\n",
    "    return df\n",
    "\n",
    "\n",
    "# as in lecture\n",
    "\"\"\"ArithmeticErrordef n_gram_similarity(df1, df2):\n",
    "    df1, df2 = set(df1), set(df2)\n",
    "    intersection = len(df1) & len(df2)\n",
    "    return 2 * intersection / len(df1) + len(df2)\"\"\"\n",
    "\n",
    "def n_gram_similarity(df1, df2):\n",
    "    set_df1 = {item for sublist in df1 for item in sublist}\n",
    "    set_df2 = {item for sublist in df2 for item in sublist}\n",
    "    intersection = len(set_df1 & set_df2)\n",
    "    return 2 * intersection / (len(set_df1) + len(set_df2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching of all initial with n_gram with two different n and treshholds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def similar_pairs_to_csv(similar_pairs, output_csv_file):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(output_csv_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(header)\n",
    "        for pair in similar_pairs:\n",
    "            writer.writerow(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.7\n",
    "n = 2\n",
    "selected_indices = ['ngram_values']\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "\n",
    "# damit können wir alle anderen matching aufrufe mit initial_..._ngram ersetzen (kann man später machen!!!\n",
    "db = initial_ngram(dblp, n, selected_columns)\n",
    "am = initial_ngram(acm, n, selected_columns)\n",
    "similairty =apply_similarity_blocks(db, am, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(similairty))\n",
    "\n",
    "# Ergebnis ist gleich wie beim unteren sim_ap funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1235\n",
      "1210\n",
      "1234\n",
      "1536\n",
      "1210\n",
      "1234\n",
      "1554\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "selected_indices = ['ngram_values']\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_ap =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_ap))\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title','year']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_apy =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_apy))\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_appv =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_appv))\n",
    "\n",
    "selected_columns = ['author_names', 'publication_venue','year']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_apvy =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_apvy))\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title', 'publication_venue','year']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_appvy =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_appvy))\n",
    "\n",
    "\n",
    "selected_columns = ['paper_title', 'publication_venue']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_ppv =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_appv))\n",
    "\n",
    "selected_columns = ['paper_title', 'publication_venue','year']\n",
    "db = initial_hash(dblp, selected_columns)\n",
    "am = initial_hash(acm, selected_columns)\n",
    "similairty_ppvy =apply_similarity_blocks(db, am, threshold, jaccard_similarity, selected_indices)\n",
    "print(len(similairty_ppvy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775\n",
      "121667\n",
      "6195\n",
      "1801\n",
      "128842\n",
      "2016\n",
      "2243\n",
      "11216\n",
      "11701\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "\n",
    "threshold = 0.7\n",
    "n = 2\n",
    "selected_indices = ['ngram_values']\n",
    "\n",
    "dblp_ap = initial_ap_ngram(dblp, n)\n",
    "acm_ap = initial_ap_ngram(acm, n)\n",
    "sim_ap = apply_similarity_blocks(dblp_ap, acm_ap, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ap))\n",
    "similar_pairs_to_csv(sim_ap, 'similarity_th_07/sim_ap.csv')\n",
    "\n",
    "dblp_apv = initial_apv_ngram(dblp, n)\n",
    "acm_apv = initial_apv_ngram(acm, n)\n",
    "sim_apv = apply_similarity_blocks(dblp_apv, acm_apv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apv))\n",
    "similar_pairs_to_csv(sim_apv, 'similarity_th_07/sim_apv.csv')\n",
    "\n",
    "dblp_ay = initial_ay_ngram(dblp, n)\n",
    "acm_ay = initial_ay_ngram(acm, n)\n",
    "sim_ay = apply_similarity_blocks(dblp_ay, acm_ay, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ay))\n",
    "similar_pairs_to_csv(sim_ay, 'similarity_th_07/sim_ay.csv')\n",
    "\n",
    "dblp_apy = initial_apy_ngram(dblp, n)\n",
    "acm_apy = initial_apy_ngram(acm, n)\n",
    "sim_apy = apply_similarity_blocks(dblp_apy, acm_apy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apy))\n",
    "similar_pairs_to_csv(sim_apy, 'similarity_th_07/sim_apy.csv')\n",
    "\n",
    "\n",
    "dblp_apvy = initial_apvy_ngram(dblp, n)\n",
    "acm_apvy = initial_apvy_ngram(acm, n)\n",
    "sim_apvy = apply_similarity_blocks(dblp_apvy, acm_apvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apvy))\n",
    "similar_pairs_to_csv(sim_apvy, 'similarity_th_07/sim_apvy.csv')\n",
    "\n",
    "dblp_appvy = initial_appvy_ngram(dblp, n)\n",
    "acm_appvy = initial_appvy_ngram(acm, n)\n",
    "sim_appvy = apply_similarity_blocks(dblp_appvy, acm_appvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_appvy))\n",
    "similar_pairs_to_csv(sim_appvy, 'similarity_th_07/sim_appvy.csv')\n",
    "\n",
    "dblp_py = initial_py_ngram(dblp, n)\n",
    "acm_py = initial_py_ngram(acm, n)\n",
    "sim_py = apply_similarity_blocks(dblp_py, acm_py, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_py))\n",
    "similar_pairs_to_csv(sim_py, 'similarity_th_07/sim_py.csv')\n",
    "\n",
    "dblp_ppv = initial_ppv_ngram(dblp, n)\n",
    "acm_ppv = initial_ppv_ngram(acm, n)\n",
    "sim_ppv = apply_similarity_blocks(dblp_ppv, acm_ppv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppv))\n",
    "similar_pairs_to_csv(sim_ppv, 'similarity_th_07/sim_ppv.csv')\n",
    "\n",
    "dblp_ppvy = initial_ppvy_ngram(dblp, n)\n",
    "acm_ppvy = initial_ppvy_ngram(acm, n)\n",
    "sim_ppvy = apply_similarity_blocks(dblp_ppvy, acm_ppvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppvy))\n",
    "similar_pairs_to_csv(sim_ppvy, 'similarity_th_07/sim_ppvy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1405\n",
      "3505\n",
      "1895\n",
      "1456\n",
      "2896\n",
      "1540\n",
      "1637\n",
      "1807\n",
      "1664\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "n = 2\n",
    "selected_indices = ['ngram_values']\n",
    "\n",
    "dblp_ap = initial_ap_ngram(dblp, n)\n",
    "acm_ap = initial_ap_ngram(acm, n)\n",
    "sim_ap2 = apply_similarity_blocks(dblp_ap, acm_ap, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ap2))\n",
    "similar_pairs_to_csv(sim_ap2, 'similarity_th_09/sim_ap.csv')\n",
    "\n",
    "dblp_apv = initial_apv_ngram(dblp, n)\n",
    "acm_apv = initial_apv_ngram(acm, n)\n",
    "sim_apv2 = apply_similarity_blocks(dblp_apv, acm_apv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apv2))\n",
    "similar_pairs_to_csv(sim_apv2, 'similarity_th_09/sim_apv.csv')\n",
    "\n",
    "dblp_ay = initial_ay_ngram(dblp, n)\n",
    "acm_ay = initial_ay_ngram(acm, n)\n",
    "sim_ay2 = apply_similarity_blocks(dblp_ay, acm_ay, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ay2))\n",
    "similar_pairs_to_csv(sim_ay2, 'similarity_th_09/sim_ay.csv')\n",
    "\n",
    "dblp_apy = initial_apy_ngram(dblp, n)\n",
    "acm_apy = initial_apy_ngram(acm, n)\n",
    "sim_apy2 = apply_similarity_blocks(dblp_apy, acm_apy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apy2))\n",
    "similar_pairs_to_csv(sim_apy2, 'similarity_th_09/sim_apy.csv')\n",
    "\n",
    "dblp_apvy = initial_apvy_ngram(dblp, n)\n",
    "acm_apvy = initial_apvy_ngram(acm, n)\n",
    "sim_apvy2 = apply_similarity_blocks(dblp_apvy, acm_apvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apvy2))\n",
    "similar_pairs_to_csv(sim_apvy2, 'similarity_th_09/sim_apvy.csv')\n",
    "\n",
    "dblp_appvy = initial_appvy_ngram(dblp, n)\n",
    "acm_appvy = initial_appvy_ngram(acm, n)\n",
    "sim_appvy2 = apply_similarity_blocks(dblp_appvy, acm_appvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_appvy2))\n",
    "similar_pairs_to_csv(sim_appvy2, 'similarity_th_09/sim_appvy.csv')\n",
    "\n",
    "dblp_py = initial_py_ngram(dblp, n)\n",
    "acm_py = initial_py_ngram(acm, n)\n",
    "sim_py2 = apply_similarity_blocks(dblp_py, acm_py, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_py2))\n",
    "similar_pairs_to_csv(sim_py2, 'similarity_th_09/sim_py.csv')\n",
    "\n",
    "dblp_ppv = initial_ppv_ngram(dblp, n)\n",
    "acm_ppv = initial_ppv_ngram(acm, n)\n",
    "sim_ppv2 = apply_similarity_blocks(dblp_ppv, acm_ppv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppv2))\n",
    "similar_pairs_to_csv(sim_ppv2, 'similarity_th_09/sim_ppv.csv')\n",
    "\n",
    "dblp_ppvy = initial_ppvy_ngram(dblp, n)\n",
    "acm_ppvy = initial_ppvy_ngram(acm, n)\n",
    "sim_ppvy2 = apply_similarity_blocks(dblp_ppvy, acm_ppvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppvy2))\n",
    "similar_pairs_to_csv(sim_ppvy2, 'similarity_th_09/sim_ppvy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620\n",
      "29923\n",
      "3897\n",
      "1670\n",
      "50182\n",
      "1771\n",
      "1917\n",
      "3927\n",
      "5339\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7\n",
    "n = 3\n",
    "selected_indices = ['ngram_values']\n",
    "\n",
    "dblp_ap = initial_ap_ngram(dblp, n)\n",
    "acm_ap = initial_ap_ngram(acm, n)\n",
    "sim_ap3 = apply_similarity_blocks(dblp_ap, acm_ap, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ap3))\n",
    "similar_pairs_to_csv(sim_ap3, 'similarity_th_07_03/sim_ap.csv')\n",
    "\n",
    "dblp_apv = initial_apv_ngram(dblp, n)\n",
    "acm_apv = initial_apv_ngram(acm, n)\n",
    "sim_apv3 = apply_similarity_blocks(dblp_apv, acm_apv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apv3))\n",
    "similar_pairs_to_csv(sim_apv3, 'similarity_th_07_03/sim_apv.csv')\n",
    "\n",
    "dblp_ay = initial_ay_ngram(dblp, n)\n",
    "acm_ay = initial_ay_ngram(acm, n)\n",
    "sim_ay3 = apply_similarity_blocks(dblp_ay, acm_ay, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ay3))\n",
    "similar_pairs_to_csv(sim_ay3, 'similarity_th_07_03/sim_ay.csv')\n",
    "\n",
    "dblp_apy = initial_apy_ngram(dblp, n)\n",
    "acm_apy = initial_apy_ngram(acm, n)\n",
    "sim_apy3 = apply_similarity_blocks(dblp_apy, acm_apy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apy3))\n",
    "similar_pairs_to_csv(sim_apy3, 'similarity_th_07_03/sim_apy.csv')\n",
    "\n",
    "dblp_apvy = initial_apvy_ngram(dblp, n)\n",
    "acm_apvy = initial_apvy_ngram(acm, n)\n",
    "sim_apvy3 = apply_similarity_blocks(dblp_apvy, acm_apvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apvy3))\n",
    "similar_pairs_to_csv(sim_apvy3, 'similarity_th_07_03/sim_apvy.csv')\n",
    "\n",
    "dblp_appvy = initial_appvy_ngram(dblp, n)\n",
    "acm_appvy = initial_appvy_ngram(acm, n)\n",
    "sim_appvy3 = apply_similarity_blocks(dblp_appvy, acm_appvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_appvy3))\n",
    "similar_pairs_to_csv(sim_appvy3, 'similarity_th_07_03/sim_appvy.csv')\n",
    "\n",
    "dblp_py = initial_py_ngram(dblp, n)\n",
    "acm_py = initial_py_ngram(acm, n)\n",
    "sim_py3 = apply_similarity_blocks(dblp_py, acm_py, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_py3))\n",
    "similar_pairs_to_csv(sim_py3, 'similarity_th_07_03/sim_py.csv')\n",
    "\n",
    "dblp_ppv = initial_ppv_ngram(dblp, n)\n",
    "acm_ppv = initial_ppv_ngram(acm, n)\n",
    "sim_ppv3 = apply_similarity_blocks(dblp_ppv, acm_ppv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppv3))\n",
    "similar_pairs_to_csv(sim_ppv3, 'similarity_th_07_03/sim_ppv.csv')\n",
    "\n",
    "dblp_ppvy = initial_ppvy_ngram(dblp, n)\n",
    "acm_ppvy = initial_ppvy_ngram(acm, n)\n",
    "sim_ppvy3 = apply_similarity_blocks(dblp_ppvy, acm_ppvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppvy3))\n",
    "similar_pairs_to_csv(sim_ppvy3, 'similarity_th_07_03/sim_ppvy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n",
      "3312\n",
      "1685\n",
      "1279\n",
      "1873\n",
      "1345\n",
      "1592\n",
      "1754\n",
      "1598\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "n = 3\n",
    "selected_indices = ['ngram_values']\n",
    "\n",
    "dblp_ap = initial_ap_ngram(dblp, n)\n",
    "acm_ap = initial_ap_ngram(acm, n)\n",
    "sim_ap4 = apply_similarity_blocks(dblp_ap, acm_ap, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ap4))\n",
    "similar_pairs_to_csv(sim_ap4, 'similarity_th_09_03/sim_ap.csv')\n",
    "\n",
    "dblp_apv = initial_apv_ngram(dblp, n)\n",
    "acm_apv = initial_apv_ngram(acm, n)\n",
    "sim_apv4 = apply_similarity_blocks(dblp_apv, acm_apv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apv4))\n",
    "similar_pairs_to_csv(sim_apv4, 'similarity_th_09_03/sim_apv.csv')\n",
    "\n",
    "dblp_ay = initial_ay_ngram(dblp, n)\n",
    "acm_ay = initial_ay_ngram(acm, n)\n",
    "sim_ay4 = apply_similarity_blocks(dblp_ay, acm_ay, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ay4))\n",
    "similar_pairs_to_csv(sim_ay4, 'similarity_th_09_03/sim_ay.csv')\n",
    "\n",
    "dblp_apy = initial_apy_ngram(dblp, n)\n",
    "acm_apy = initial_apy_ngram(acm, n)\n",
    "sim_apy4 = apply_similarity_blocks(dblp_apy, acm_apy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apy4))\n",
    "similar_pairs_to_csv(sim_apy4, 'similarity_th_09_03/sim_apy.csv')\n",
    "\n",
    "dblp_apvy = initial_apvy_ngram(dblp, n)\n",
    "acm_apvy = initial_apvy_ngram(acm, n)\n",
    "sim_apvy4 = apply_similarity_blocks(dblp_apvy, acm_apvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_apvy4))\n",
    "similar_pairs_to_csv(sim_apvy4, 'similarity_th_09_03/sim_apvy.csv')\n",
    "\n",
    "dblp_appvy = initial_appvy_ngram(dblp, n)\n",
    "acm_appvy = initial_appvy_ngram(acm, n)\n",
    "sim_appvy4 = apply_similarity_blocks(dblp_appvy, acm_appvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_appvy4))\n",
    "similar_pairs_to_csv(sim_appvy4, 'similarity_th_09_03/sim_appvy.csv')\n",
    "\n",
    "dblp_py = initial_py_ngram(dblp, n)\n",
    "acm_py = initial_py_ngram(acm, n)\n",
    "sim_py4 = apply_similarity_blocks(dblp_py, acm_py, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_py4))\n",
    "similar_pairs_to_csv(sim_py4, 'similarity_th_09_03/sim_py.csv')\n",
    "\n",
    "dblp_ppv = initial_ppv_ngram(dblp, n)\n",
    "acm_ppv = initial_ppv_ngram(acm, n)\n",
    "sim_ppv4 = apply_similarity_blocks(dblp_ppv, acm_ppv, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppv4))\n",
    "similar_pairs_to_csv(sim_ppv4, 'similarity_th_09_03/sim_ppv.csv')\n",
    "\n",
    "dblp_ppvy = initial_ppvy_ngram(dblp, n)\n",
    "acm_ppvy = initial_ppvy_ngram(acm, n)\n",
    "sim_ppvy4 = apply_similarity_blocks(dblp_ppvy, acm_ppvy, threshold, n_gram_similarity, selected_indices)\n",
    "print(len(sim_ppvy4))\n",
    "similar_pairs_to_csv(sim_ppvy4, 'similarity_th_09_03/sim_ppvy.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bemerkung: Wir haben einige Outliner mit Matches mit einer Anzahl von ca 30k-120k Paaren haben. Die Ursache hierbei liegt beim Prozess und die Werte: Beispielsweise nehmen wir bei den initialen Methoden das ganze Jahr und für publication venue extrahieren wir sigmod oder vdlb. Da es beim venue nur die zwei Möglichkeiten gibt kriegen wir für die Ähnlichkeit entweder 1 (identisch) oder x und beim Jahr haben wir 1995-2004 bedeutet bei identischen 1, bei ähnlichen 1995-1999 oder 2000-2004 kriegt man ein hohen Wert.\n",
    "Natürlich werden die Werte nicht einzeln, sondern als kombinierter String verglichen, aber wenn man nicht genügend andere Werte oder lange = (Anzahl an Characters) Werte hat wie paper_title, dann wird es eben als sehr ähnlich gewertet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching with hashblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1730\n",
      "1116\n",
      "1172\n",
      "1056\n",
      "1057\n",
      "1184\n",
      "1058\n",
      "1480\n",
      "1489\n",
      "1490\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "# make it so it either sigmod or vdlb (could have been done for initial_ngrams too but to lazy to change now)\n",
    "dblp['publication_venue'] = dblp['publication_venue'].apply(lambda x: 'sigmod' if 'sigmod' in x else 'vldb')\n",
    "acm['publication_venue'] = acm['publication_venue'].apply(lambda x: 'sigmod' if 'sigmod' in x else 'vldb')\n",
    "\n",
    "threshold = 0.65\n",
    "indices = ['hash_value']\n",
    "set_indices = set(indices)\n",
    "\n",
    "blocking_columns = ['author_names','paper_title']\n",
    "dblp_hap = hash_blocking(dblp, blocking_columns)\n",
    "acm_hap = hash_blocking(acm,blocking_columns)\n",
    "sim_hap = apply_similarity_blocks(dblp_hap, acm_hap, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_ap))\n",
    "similar_pairs_to_csv(sim_hap, 'similarity_hash_0.65/sim.hap.csv')\n",
    "\n",
    "blocking_columns = ['author_names','publication_venue']\n",
    "dblp_hapv = hash_blocking(dblp, blocking_columns)\n",
    "acm_hapv = hash_blocking(acm,blocking_columns)\n",
    "sim_hapv = apply_similarity_blocks(dblp_hapv, acm_hapv, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hapv))\n",
    "similar_pairs_to_csv(sim_hapv, 'similarity_hash_0.65/sim.hapv.csv')\n",
    "\n",
    "blocking_columns = ['author_names','year']\n",
    "dblp_hay = hash_blocking(dblp, blocking_columns)\n",
    "acm_hay = hash_blocking(acm,blocking_columns)\n",
    "sim_hay = apply_similarity_blocks(dblp_hay, acm_hay, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hay))\n",
    "similar_pairs_to_csv(sim_hay, 'similarity_hash_0.65/sim.hay.csv')\n",
    "\n",
    "blocking_columns = ['author_names','paper_title', 'publication_venue']\n",
    "dblp_happv = hash_blocking(dblp, blocking_columns)\n",
    "acm_happv = hash_blocking(acm,blocking_columns)\n",
    "sim_happv = apply_similarity_blocks(dblp_happv, acm_happv, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_happv))\n",
    "similar_pairs_to_csv(sim_happv, 'similarity_hash_0.65/sim.happv.csv')\n",
    "\n",
    "blocking_columns = ['author_names','paper_title', 'year']\n",
    "dblp_hapy = hash_blocking(dblp, blocking_columns)\n",
    "acm_hapy = hash_blocking(acm,blocking_columns)\n",
    "sim_hapy = apply_similarity_blocks(dblp_hapy, acm_hapy, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hapy))\n",
    "similar_pairs_to_csv(sim_hapy, 'similarity_hash_0.65/sim.hapy.csv')\n",
    "\n",
    "blocking_columns = ['author_names', 'publication_venue','year']\n",
    "dblp_hapvy = hash_blocking(dblp, blocking_columns)\n",
    "acm_hapvy = hash_blocking(acm,blocking_columns)\n",
    "sim_hapvy = apply_similarity_blocks(dblp_hapvy, acm_hapvy, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hapvy))\n",
    "similar_pairs_to_csv(sim_hapvy, 'similarity_hash_0.65/sim.hapvy.csv')\n",
    "\n",
    "blocking_columns = ['author_names','paper_title', 'publication_venue','year']\n",
    "dblp_happy = hash_blocking(dblp, blocking_columns)\n",
    "acm_happy = hash_blocking(acm,blocking_columns)\n",
    "sim_happy = apply_similarity_blocks(dblp_happy, acm_happy, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_happy))\n",
    "similar_pairs_to_csv(sim_happy, 'similarity_hash_0.65/sim.happy.csv')\n",
    "\n",
    "blocking_columns = ['paper_title', 'publication_venue']\n",
    "dblp_hppv = hash_blocking(dblp, blocking_columns)\n",
    "acm_hppv = hash_blocking(acm,blocking_columns)\n",
    "sim_hppv = apply_similarity_blocks(dblp_hppv, acm_hppv, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hppv))\n",
    "similar_pairs_to_csv(sim_hppv, 'similarity_hash_0.65/sim.hppv.csv')\n",
    "\n",
    "blocking_columns = ['paper_title', 'year']\n",
    "dblp_hpy = hash_blocking(dblp, blocking_columns)\n",
    "acm_hpy = hash_blocking(acm,blocking_columns)\n",
    "sim_hpy = apply_similarity_blocks(dblp_hpy, acm_hpy, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hpy))\n",
    "similar_pairs_to_csv(sim_hpy, 'similarity_hash_0.65/sim.hppvy.csv')\n",
    "\n",
    "blocking_columns = ['paper_title', 'publication_venue','year']\n",
    "dblp_hppvy = hash_blocking(dblp, blocking_columns)\n",
    "acm_hppvy = hash_blocking(acm,blocking_columns)\n",
    "sim_hppvy = apply_similarity_blocks(dblp_hppvy, acm_hppvy, threshold, jaccard_similarity, set_indices)\n",
    "print(len(sim_hppvy))\n",
    "similar_pairs_to_csv(sim_hppvy, 'similarity_hash_0.65/sim.hppvy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def ngrams_tuple(s, n):\n",
    "    return [tuple(s[i:i+n]) for i in range(len(s)-n+1)]\n",
    "\n",
    "def hash_ngram_values(ngram_values):\n",
    "    return hashlib.sha256(str(hash(tuple(sorted(ngram_values.items())))).encode()).hexdigest()\n",
    "\n",
    "def n_gram_blocking(dataframe, columns, n):\n",
    "    # n-gram for each selected column \n",
    "    for column in columns:\n",
    "        key_name = f'ngram_key_{column}'\n",
    "        dataframe[key_name] = dataframe[column].apply(lambda x: ''.join(map(str, x)))\n",
    "\n",
    "        ngram_col_name = f'ngram_values_{column}'\n",
    "        dataframe[ngram_col_name] = dataframe[key_name].apply(lambda x: tuple(ngrams_tuple(x, n)))\n",
    "\n",
    "    # block them and each block consists of n-gram values and the respective index (id) \n",
    "    blocks = {}\n",
    "    for index, row in dataframe.iterrows():\n",
    "        ngram_values = {column: row[f'ngram_values_{column}'] for column in columns}\n",
    "        hashable_ngram_values = hash_ngram_values(ngram_values)\n",
    "        \n",
    "        if hashable_ngram_values in blocks:\n",
    "            blocks[hashable_ngram_values]['index'].add(row['index'])\n",
    "        else:\n",
    "            blocks[hashable_ngram_values] = {'index': {row['index']}, **ngram_values}\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664\n",
      "2463\n"
     ]
    }
   ],
   "source": [
    "dblp_csv = 'CSV-files/dblp.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "acm_csv = 'CSV-files/acm.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "n = 2\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "\n",
    "# bei 0.5 = 0 gemeinsame paare und probiere nur dieses Beispiel, weil es lange dauert\n",
    "threshold = 0.3\n",
    "\n",
    "db = n_gram_blocking(dblp, selected_columns,n)\n",
    "ac = n_gram_blocking(acm, selected_columns, n)\n",
    "ap_sim = apply_similarity_blocks(db, ac, threshold, n_gram_similarity, selected_columns)\n",
    "print(len(ap_sim))\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "\n",
    "threshold = 0.7\n",
    "sim = apply_similarity_blocks(db, ac, threshold, n_gram_similarity, selected_columns)\n",
    "print(len(sim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode braucht nur die jeweiligen Match methoden: Verwende eins als Baseline und das andere als Comparison. Müssten uns für eine Match methode entscheiden lass z.B sagen similarity_appvy (a = author, p = paper_title, pv= publication_venue, y = year) als Baseline und vergleichbare Matching-Methoden (alle anderen initials similarities) evaluieren. Natürlich kann man auch die gleiche Methode, aber mit anderen n (n-gram) wert und treshold vergleichen. Müssten uns für eine Baseline entscheiden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9779785431959345, 0.8686058174523571, 0.9200531208499335)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_similarity(baseline, comparison):\n",
    "    baseline_set, comparison_set = set(baseline), set(comparison)\n",
    "\n",
    "    tp = len(baseline_set.intersection(comparison_set))\n",
    "    fp = len(comparison_set - baseline_set)\n",
    "    fn = len(baseline_set - comparison_set)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "print(evaluate_similarity(sim_appvy, sim_ap))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo = - Eventuell weitere Blocking Methoden\n",
    "       - kombinieren mit block ranges, welches Dataframe nach Jahren und publisher aufteilt in blöcke und dann jedes der Blöcke zusätlich mit einem anderen blocking oder direkt vergelichen\n",
    "       - Aufgabe 2.3) die gewählte Baseline als CSV und kmeans darauf !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
