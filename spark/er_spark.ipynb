{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/28 23:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'5', '6', '4'}, {'3', '2', '1'}]\n"
     ]
    }
   ],
   "source": [
    "import hash_blocking_spark as hash\n",
    "import ngram_blocking_spark as ngram\n",
    "import matchers_spark as match\n",
    "import similarity_spark as sim\n",
    "import blocking_structured_and_sorted_spark as ss\n",
    "import cluster_spark as cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel nehme unser Match, cluster mit Spark und schreibe das Ergebnis von Spark als CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "pairs = reconstructed_pairs('/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Matched Entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "cluster_pairs = cluster.build_clusters_parallel(pairs)\n",
    "cluster_to_csv(cluster_pairs, '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv')\n",
    "print(len(cluster_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 1.8495590686798096 seconds. Number of index combinations: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x107669690>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.n_gram_blocking(acm_df, selected_columns,2)\n",
    "d = ngram.n_gram_blocking(dblp_df, selected_columns,2)\n",
    "e = match.apply_ngram_blocks_spark(c,d, 0.7, sim.jaccard_similarity_ngrams)\n",
    "\n",
    "\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 1.8713901042938232 seconds. Number of index combinations: 782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11b67c460>>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.initial_ngram_parallel_df(spark,acm_df, selected_columns,2)\n",
    "d = ngram.initial_ngram_parallel_df(spark, dblp_df, selected_columns,2)\n",
    "e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "#e = apply_similarity_blocks_spark(c, d, 0.1, sim.jaccard_similarity_ngrams)\n",
    "\n",
    "spark.stop\n",
    "#db = ngram.initial_ngram_parallel(dblp_csv, selected_columns, 2)\n",
    "\n",
    "#simo = match.apply_similarity_blocks_spark(ap, db, 0.7, hash_indices, sim.jaccard_similarity_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index1': '[53908b4920f70186a0dbacb7]', 'index2': '[53e9a297b7602d9702b9cf03]', 'similarity': '1.0'}\n",
      "{'index1': '[5390958a20f70186a0df0138]', 'index2': '[53e9af2db7602d9703969dc1]', 'similarity': '1.0'}\n",
      "{'index1': '[53908a9620f70186a0da4d9c]', 'index2': '[53e9b98ab7602d970457bd03]', 'similarity': '1.0'}\n",
      "{'index1': '539087f320f70186a0d6f876', 'index2': '53e9ad04b7602d97036e13e5', 'similarity': '1.0'}\n",
      "{'index1': '53908b4920f70186a0dbb083', 'index2': '53e9ad04b7602d97036e13e5', 'similarity': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    {'index1': '[53908b4920f70186a0dbacb7]', 'index2': '[53e9a297b7602d9702b9cf03]', 'similarity': '1.0'},\n",
    "    {'index1': '[5390958a20f70186a0df0138]', 'index2': '[53e9af2db7602d9703969dc1]', 'similarity': '1.0'},\n",
    "    {'index1': '[53908a9620f70186a0da4d9c]', 'index2': '[53e9b98ab7602d970457bd03]', 'similarity': '1.0'},\n",
    "    {'index1': '[539087f320f70186a0d6f876, 53908b4920f70186a0dbb083]', 'index2': '[53e9ad04b7602d97036e13e5]', 'similarity': '1.0'},\n",
    "    # Add more rows here\n",
    "]\n",
    "\n",
    "# Initialize a list to store processed rows\n",
    "processed_data = []\n",
    "\n",
    "# Iterate through each row\n",
    "for row in data:\n",
    "    # Split the index values and remove brackets\n",
    "    index1_values = row['index1'][1:-1].split(', ')\n",
    "    index2_values = row['index2'][1:-1].split(', ')\n",
    "\n",
    "    # Check if either index has multiple values\n",
    "    if len(index1_values) > 1 or len(index2_values) > 1:\n",
    "        # Generate combinations of indices\n",
    "        index_combinations = list(product(index1_values, index2_values))\n",
    "\n",
    "        # Create a new row for each combination\n",
    "        for combination in index_combinations:\n",
    "            processed_row = {\n",
    "                'index1': combination[0],\n",
    "                'index2': combination[1],\n",
    "                'similarity': row['similarity']\n",
    "            }\n",
    "            processed_data.append(processed_row)\n",
    "    else:\n",
    "        # If no index has multiple values, simply append the row\n",
    "        processed_data.append(row)\n",
    "\n",
    "# Print the processed data\n",
    "for row in processed_data:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define your pairs\n",
    "pairs = [\n",
    "    {'index1': [1], 'index2': [3]},\n",
    "    {'index1': [2], 'index2': [4]}\n",
    "\n",
    "]\n",
    "\n",
    "combined_pairs = []\n",
    "\n",
    "# Iterate through each pair\n",
    "for pair in pairs:\n",
    "    # Generate combinations of index1 and index2 values\n",
    "    combinations = list(product(pair['index1'], pair['index2']))\n",
    "\n",
    "    # Append combinations to the combined_pairs list\n",
    "    combined_pairs.extend(combinations)\n",
    "\n",
    "print(combined_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType\n",
    "def apply_similarity_blocks_spark(blocks1, blocks2, threshold, field_name, similarity_function):\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    df1 = spark.read.csv(blocks1, header=True, inferSchema=True)\n",
    "    df2 = spark.read.csv(blocks2, header=True, inferSchema=True)\n",
    "    data_tuples1 = [(key, value[field_name], value['index']) for key, value in blocks1.items()]\n",
    "    data_tuples2 = [(key, value[field_name], value['index']) for key, value in blocks2.items()]\n",
    "    \n",
    "    schema = StructType([       \n",
    "        StructField(\"blocking_key\", StringType(), nullable=False),\n",
    "        StructField(field_name, StringType(), nullable=False),\n",
    "        StructField(\"index\", ArrayType(StringType()), nullable=False)\n",
    "    ])\n",
    "    \n",
    "    blocks1_df = spark.createDataFrame(data_tuples1, schema=schema)\n",
    "    blocks2_df = spark.createDataFrame(data_tuples2, schema=schema)\n",
    "  \n",
    "   \n",
    "    similarity_udf = F.udf(lambda set1, set2: str(similarity_function(set(set1), set(set2))), StringType())\n",
    "\n",
    "    joined_blocks = df1.alias(\"block1\").crossJoin(df2.alias(\"block2\"))\n",
    "    similar_pairs_df = joined_blocks.where(\n",
    "        (F.col(\"block1.value\") == F.col(\"block2.value\"))\n",
    "    ).select(\n",
    "        F.col(\"block1.index\").alias(\"index1\"),\n",
    "        F.col(\"block2.index\").alias(\"index2\"),\n",
    "        similarity_udf(F.col(\"block1.value\"), F.col(\"block2.value\")).alias(\"similarity\")\n",
    "    )\n",
    "\n",
    "    similar_pairs_df = similar_pairs_df.filter(F.col(\"similarity\") >= threshold)\n",
    "    similar_pairs = similar_pairs_df.collect()\n",
    "    \n",
    "    index_combinations = [(i, j) for pair in similar_pairs for i in pair['index1'] for j in pair['index2']]\n",
    "    index_combinations = list(set(index_combinations))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing time: {elapsed_time} seconds. Number of index combinations: {len(similar_pairs)}\")\n",
    "\n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = hash.initial_hash_parallel(spark, acm_df, selected_columns)\n",
    "d = hash.initial_hash_parallel(spark, dblp_df, selected_columns)\n",
    "e = match.apply_similarity_blocks_spark(c, d, 0.7, sim.jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "ap = ss.block_by_year_and_publisher(acm_csv, year_block, labels)\n",
    "db = ss.block_by_year_and_publisher(dblp_csv, year_block, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
