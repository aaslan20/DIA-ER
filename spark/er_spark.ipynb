{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hash_blocking_spark as hash\n",
    "import ngram_blocking_spark as ngram\n",
    "import matchers_spark as match\n",
    "import similarity_spark as sim\n",
    "import blocking_structured_and_sorted_spark as ss\n",
    "import cluster_spark as cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel nehme unser Match, cluster mit Spark und schreibe das Ergebnis von Spark als CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "pairs = reconstructed_pairs('../Matched/Matched Entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "cluster_pairs = cluster.build_clusters_parallel(pairs)\n",
    "cluster_to_csv(cluster_pairs, '../Matched/Spark_Cluster.csv')\n",
    "print(len(cluster_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 13.989503860473633 seconds. Number of index combinations: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x0000021E98713FD0>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.n_gram_blocking(acm_df, selected_columns,2)\n",
    "d = ngram.n_gram_blocking(dblp_df, selected_columns,2)\n",
    "e = match.apply_ngram_blocks_spark(c,d, 0.7, sim.jaccard_similarity_ngrams)\n",
    "\n",
    "\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dblp_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(dblp_csv, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_ngram_parallel_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43macm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m d \u001b[38;5;241m=\u001b[39m ngram\u001b[38;5;241m.\u001b[39minitial_ngram_parallel_df(spark, dblp_df, selected_columns,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     10\u001b[0m e \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mapply_similarity_blocks_spark(c,d, \u001b[38;5;241m0.7\u001b[39m, sim\u001b[38;5;241m.\u001b[39mjaccard_similarity)\n",
      "File \u001b[1;32mc:\\Users\\sechs\\Documents\\Projects\\DIA-ER\\spark\\ngram_blocking_spark.py:20\u001b[0m, in \u001b[0;36minitial_ngram_parallel_df\u001b[1;34m(df, columns_to_use, spark, n)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_ngram_parallel_df\u001b[39m(df, columns_to_use, spark, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# wenn du dataframe direkt packst entferne das und statt path einfach df als parameter\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_venue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns_to_use\u001b[49m:\n\u001b[0;32m     21\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mwhen(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvldb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     23\u001b[0m     transform_author_names_udf \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mudf(\u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(name\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39msplit()]), StringType())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py:1369\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert column into bool: please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when building DataFrame boolean expressions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.initial_ngram_parallel_df(spark,acm_df, selected_columns,2)\n",
    "d = ngram.initial_ngram_parallel_df(spark, dblp_df, selected_columns,2)\n",
    "e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "#e = apply_similarity_blocks_spark(c, d, 0.1, sim.jaccard_similarity_ngrams)\n",
    "\n",
    "spark.stop\n",
    "#db = ngram.initial_ngram_parallel(dblp_csv, selected_columns, 2)\n",
    "\n",
    "#simo = match.apply_similarity_blocks_spark(ap, db, 0.7, hash_indices, sim.jaccard_similarity_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index1': '[53908b4920f70186a0dbacb7]', 'index2': '[53e9a297b7602d9702b9cf03]', 'similarity': '1.0'}\n",
      "{'index1': '[5390958a20f70186a0df0138]', 'index2': '[53e9af2db7602d9703969dc1]', 'similarity': '1.0'}\n",
      "{'index1': '[53908a9620f70186a0da4d9c]', 'index2': '[53e9b98ab7602d970457bd03]', 'similarity': '1.0'}\n",
      "{'index1': '539087f320f70186a0d6f876', 'index2': '53e9ad04b7602d97036e13e5', 'similarity': '1.0'}\n",
      "{'index1': '53908b4920f70186a0dbb083', 'index2': '53e9ad04b7602d97036e13e5', 'similarity': '1.0'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    {'index1': '[53908b4920f70186a0dbacb7]', 'index2': '[53e9a297b7602d9702b9cf03]', 'similarity': '1.0'},\n",
    "    {'index1': '[5390958a20f70186a0df0138]', 'index2': '[53e9af2db7602d9703969dc1]', 'similarity': '1.0'},\n",
    "    {'index1': '[53908a9620f70186a0da4d9c]', 'index2': '[53e9b98ab7602d970457bd03]', 'similarity': '1.0'},\n",
    "    {'index1': '[539087f320f70186a0d6f876, 53908b4920f70186a0dbb083]', 'index2': '[53e9ad04b7602d97036e13e5]', 'similarity': '1.0'},\n",
    "    # Add more rows here\n",
    "]\n",
    "\n",
    "# Initialize a list to store processed rows\n",
    "processed_data = []\n",
    "\n",
    "# Iterate through each row\n",
    "for row in data:\n",
    "    # Split the index values and remove brackets\n",
    "    index1_values = row['index1'][1:-1].split(', ')\n",
    "    index2_values = row['index2'][1:-1].split(', ')\n",
    "\n",
    "    # Check if either index has multiple values\n",
    "    if len(index1_values) > 1 or len(index2_values) > 1:\n",
    "        # Generate combinations of indices\n",
    "        index_combinations = list(product(index1_values, index2_values))\n",
    "\n",
    "        # Create a new row for each combination\n",
    "        for combination in index_combinations:\n",
    "            processed_row = {\n",
    "                'index1': combination[0],\n",
    "                'index2': combination[1],\n",
    "                'similarity': row['similarity']\n",
    "            }\n",
    "            processed_data.append(processed_row)\n",
    "    else:\n",
    "        # If no index has multiple values, simply append the row\n",
    "        processed_data.append(row)\n",
    "\n",
    "# Print the processed data\n",
    "for row in processed_data:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define your pairs\n",
    "pairs = [\n",
    "    {'index1': [1], 'index2': [3]},\n",
    "    {'index1': [2], 'index2': [4]}\n",
    "\n",
    "]\n",
    "\n",
    "combined_pairs = []\n",
    "\n",
    "# Iterate through each pair\n",
    "for pair in pairs:\n",
    "    # Generate combinations of index1 and index2 values\n",
    "    combinations = list(product(pair['index1'], pair['index2']))\n",
    "\n",
    "    # Append combinations to the combined_pairs list\n",
    "    combined_pairs.extend(combinations)\n",
    "\n",
    "print(combined_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType\n",
    "def apply_similarity_blocks_spark(blocks1, blocks2, threshold, field_name, similarity_function):\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    df1 = spark.read.csv(blocks1, header=True, inferSchema=True)\n",
    "    df2 = spark.read.csv(blocks2, header=True, inferSchema=True)\n",
    "    data_tuples1 = [(key, value[field_name], value['index']) for key, value in blocks1.items()]\n",
    "    data_tuples2 = [(key, value[field_name], value['index']) for key, value in blocks2.items()]\n",
    "    \n",
    "    schema = StructType([       \n",
    "        StructField(\"blocking_key\", StringType(), nullable=False),\n",
    "        StructField(field_name, StringType(), nullable=False),\n",
    "        StructField(\"index\", ArrayType(StringType()), nullable=False)\n",
    "    ])\n",
    "    \n",
    "    blocks1_df = spark.createDataFrame(data_tuples1, schema=schema)\n",
    "    blocks2_df = spark.createDataFrame(data_tuples2, schema=schema)\n",
    "  \n",
    "   \n",
    "    similarity_udf = F.udf(lambda set1, set2: str(similarity_function(set(set1), set(set2))), StringType())\n",
    "\n",
    "    joined_blocks = df1.alias(\"block1\").crossJoin(df2.alias(\"block2\"))\n",
    "    similar_pairs_df = joined_blocks.where(\n",
    "        (F.col(\"block1.value\") == F.col(\"block2.value\"))\n",
    "    ).select(\n",
    "        F.col(\"block1.index\").alias(\"index1\"),\n",
    "        F.col(\"block2.index\").alias(\"index2\"),\n",
    "        similarity_udf(F.col(\"block1.value\"), F.col(\"block2.value\")).alias(\"similarity\")\n",
    "    )\n",
    "\n",
    "    similar_pairs_df = similar_pairs_df.filter(F.col(\"similarity\") >= threshold)\n",
    "    similar_pairs = similar_pairs_df.collect()\n",
    "    \n",
    "    index_combinations = [(i, j) for pair in similar_pairs for i in pair['index1'] for j in pair['index2']]\n",
    "    index_combinations = list(set(index_combinations))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing time: {elapsed_time} seconds. Number of index combinations: {len(similar_pairs)}\")\n",
    "\n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 7.17189359664917 seconds. Number of index combinations: 782\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = hash.initial_hash_parallel(acm_df, selected_columns, spark)\n",
    "d = hash.initial_hash_parallel(dblp_df, selected_columns, spark)\n",
    "e = match.apply_similarity_blocks_spark(c, d, 0.7, sim.jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_block = [1995,1996,1997, 1998, 1999,2000,2001, 2002, 2003, 2004,2005]\n",
    "labels = [\"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\"]\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "ap = ss.block_by_year_and_publisher(acm_csv, year_block, labels)\n",
    "db = ss.block_by_year_and_publisher(dblp_csv, year_block, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "c = hash.initial_hash_parallel(acm_df, selected_columns, spark)\n",
    "d = hash.initial_hash_parallel(dblp_df, selected_columns, spark)\n",
    "e = match.apply_similarity_blocks_spark(c, d, 0.7, sim.jaccard_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
