{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hash_blocking_spark as hash\n",
    "import ngram_blocking_spark as ngram\n",
    "import matchers_spark as match\n",
    "import similarity_spark as sim\n",
    "import blocking_structured_and_sorted_spark as ss\n",
    "import cluster_spark as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/30 15:42:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 2.375669002532959 seconds. Number of index combinations: 1052\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# this was in our experiment in two the best match\n",
    "a = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "b = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "c = match.apply_similarity_blocks_spark(a,b, 0.7, sim.jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_index_pairs_to_csv(data, filename):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Write the header\n",
    "        for row in data:\n",
    "            cleaned_row = [row[0].strip('[]'), row[1].strip('[]')]  # Remove brackets\n",
    "            writer.writerow(cleaned_row)\n",
    "\n",
    "write_index_pairs_to_csv(c, '../Matched/Matched_spark.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matched entites as a list back for spark and without\n",
    "import pandas as pd\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "pairs = reconstructed_pairs('../Matched/Matched Entities.csv')\n",
    "pairs_spark = reconstructed_pairs('../Matched/Matched_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are equal\n",
    "def pairs_correspond(pair_list1, pair_list2):\n",
    "    sorted_pair_list1 = [tuple(sorted(pair)) for pair in pair_list1]\n",
    "    sorted_pair_list2 = [tuple(sorted(pair)) for pair in pair_list2]\n",
    "\n",
    "    for pair1 in sorted_pair_list1:\n",
    "        if pair1 not in sorted_pair_list2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "pairs_correspond(pairs_spark, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 15:48:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# cluster the spark pairs which are equals\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "cluster_pairs = cluster.build_clusters_parallel(pairs_spark)\n",
    "cluster_to_csv(cluster_pairs, '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv')\n",
    "print(len(cluster_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 2.2450268268585205 seconds. Number of index combinations: 1052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11b3e8310>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "    acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "    dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "    \n",
    "    c = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "    d = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "    e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "    spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dblp_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(dblp_csv, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_ngram_parallel_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43macm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m d \u001b[38;5;241m=\u001b[39m ngram\u001b[38;5;241m.\u001b[39minitial_ngram_parallel_df(spark, dblp_df, selected_columns,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     10\u001b[0m e \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mapply_similarity_blocks_spark(c,d, \u001b[38;5;241m0.7\u001b[39m, sim\u001b[38;5;241m.\u001b[39mjaccard_similarity)\n",
      "File \u001b[1;32mc:\\Users\\sechs\\Documents\\Projects\\DIA-ER\\spark\\ngram_blocking_spark.py:20\u001b[0m, in \u001b[0;36minitial_ngram_parallel_df\u001b[1;34m(df, columns_to_use, spark, n)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_ngram_parallel_df\u001b[39m(df, columns_to_use, spark, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# wenn du dataframe direkt packst entferne das und statt path einfach df als parameter\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_venue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns_to_use\u001b[49m:\n\u001b[0;32m     21\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mwhen(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvldb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     23\u001b[0m     transform_author_names_udf \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mudf(\u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(name\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39msplit()]), StringType())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py:1369\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert column into bool: please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when building DataFrame boolean expressions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 1.8916327953338623 seconds. Number of index combinations: 782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11b3e8310>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example that other are functioning\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.initial_ngram_parallel_df(spark,acm_df, selected_columns,2)\n",
    "d = ngram.initial_ngram_parallel_df(spark, dblp_df, selected_columns,2)\n",
    "e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
