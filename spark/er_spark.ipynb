{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hash_blocking_spark as hash\n",
    "import ngram_blocking_spark as ngram\n",
    "import matchers_spark as match\n",
    "import similarity_spark as sim\n",
    "import blocking_structured_and_sorted_spark as ss\n",
    "import cluster_spark as cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 32.16172432899475 seconds. Number of index combinations: 1052\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "\n",
    "selected_columns = ['paper_title']\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# this was in our experiment in two the best match\n",
    "a = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "b = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "c = match.apply_similarity_blocks_spark(a,b, 0.7, sim.jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_index_pairs_to_csv(data, filename):\n",
    "    header = ['dblp_index', 'acm_index']\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Write the header\n",
    "        for row in data:\n",
    "            cleaned_row = [row[0].strip('[]'), row[1].strip('[]')]  # Remove brackets\n",
    "            writer.writerow(cleaned_row)\n",
    "\n",
    "write_index_pairs_to_csv(c, '../Matched/Matched_spark.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get matched entites as a list back for spark and without\n",
    "import pandas as pd\n",
    "def reconstructed_pairs(path):\n",
    "    df_pairs = pd.read_csv(path)\n",
    "    return list(zip(df_pairs['dblp_index'], df_pairs['acm_index']))\n",
    "\n",
    "pairs = reconstructed_pairs('../Matched/Matched Entities.csv')\n",
    "pairs_spark = reconstructed_pairs('../Matched/Matched_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are equal\n",
    "def pairs_correspond(pair_list1, pair_list2):\n",
    "    sorted_pair_list1 = [tuple(sorted(pair)) for pair in pair_list1]\n",
    "    sorted_pair_list2 = [tuple(sorted(pair)) for pair in pair_list2]\n",
    "\n",
    "    for pair1 in sorted_pair_list1:\n",
    "        if pair1 not in sorted_pair_list2:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "pairs_correspond(pairs_spark, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m cluster_pairs \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mbuild_clusters_parallel(pairs_spark)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mcluster_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(cluster_pairs))\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mcluster_to_csv\u001b[1;34m(cluster_data, filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster_to_csv\u001b[39m(cluster_data, filename):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m      5\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile)\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cluster_data, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# cluster the spark pairs which are equals\n",
    "def cluster_to_csv(cluster_data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for idx, item in enumerate(cluster_data, start=1):\n",
    "            writer.writerow([f'Cluster {idx}: {item}'])\n",
    "\n",
    "cluster_pairs = cluster.build_clusters_parallel(pairs_spark)\n",
    "cluster_to_csv(cluster_pairs, '/Users/aliaslan/Documents/GitHub/DIA-ER/Matched/Spark_Cluster.csv')\n",
    "print(len(cluster_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 33.05843901634216 seconds. Number of index combinations: 1052\n",
      "Blocking paper_title with identity and lower resulted in 1052 matches.\n",
      "Processing time: 34.25920796394348 seconds. Number of index combinations: 1052\n",
      "Blocking paper_title with identity and title resulted in 1052 matches.\n",
      "Processing time: 37.75804615020752 seconds. Number of index combinations: 1052\n",
      "Blocking paper_title with identity and alphanumerical resulted in 1052 matches.\n",
      "Processing time: 35.127676010131836 seconds. Number of index combinations: 1052\n",
      "Blocking paper_title with identity and reverse resulted in 1052 matches.\n",
      "Processing time: 34.83160614967346 seconds. Number of index combinations: 1052\n",
      "Blocking paper_title with lower and title resulted in 1052 matches.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lower, reverse, col\n",
    "from pyspark.sql.types import StringType\n",
    "from itertools import combinations\n",
    "\n",
    "dblp_csv = '../CSV-files/dblp_stem.csv'\n",
    "dblp = pd.read_csv(dblp_csv)\n",
    "\n",
    "acm_csv = '../CSV-files/acm_stem.csv'\n",
    "acm = pd.read_csv(acm_csv)\n",
    "selected_columns = ['paper_title']\n",
    "\n",
    "@udf(StringType())\n",
    "def alphanumerical(entity: str) -> str:\n",
    "    return ''.join([char for char in entity if char.isalnum()])\n",
    "@udf(StringType())\n",
    "def title(entity: str) -> str:\n",
    "    return entity.title()\n",
    "@udf(StringType())\n",
    "def identity(entity: str) -> str:\n",
    "    return entity\n",
    "\n",
    "augementations = [identity, lower, title, alphanumerical, reverse]\n",
    "for i in range(1, len(augementations)):\n",
    "    print(f\"Blocking paper_title with {i} augmentations.\\n\")\n",
    "    for comb in enumerate(combinations(augementations, i)):\n",
    "        spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "        acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "        dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "        dblp_result_df = dblp_df.withColumn(\"paper_title\", comb[1](comb[0](col(\"paper_title\"))))\n",
    "        acm_result_df = acm_df.withColumn(\"paper_title\", comb[1](comb[0](col(\"paper_title\"))))\n",
    "        c = hash.initial_hash_parallel_df(spark,acm_df, selected_columns)\n",
    "        d = hash.initial_hash_parallel_df(spark, dblp_df, selected_columns)\n",
    "        e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "        print(f\"Blocking paper_title with {comb[0].__name__} and {comb[1].__name__} resulted in {len(e)} matches.\")\n",
    "        spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dblp_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(dblp_csv, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 8\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mngram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_ngram_parallel_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43macm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m d \u001b[38;5;241m=\u001b[39m ngram\u001b[38;5;241m.\u001b[39minitial_ngram_parallel_df(spark, dblp_df, selected_columns,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     10\u001b[0m e \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mapply_similarity_blocks_spark(c,d, \u001b[38;5;241m0.7\u001b[39m, sim\u001b[38;5;241m.\u001b[39mjaccard_similarity)\n",
      "File \u001b[1;32mc:\\Users\\sechs\\Documents\\Projects\\DIA-ER\\spark\\ngram_blocking_spark.py:20\u001b[0m, in \u001b[0;36minitial_ngram_parallel_df\u001b[1;34m(df, columns_to_use, spark, n)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_ngram_parallel_df\u001b[39m(df, columns_to_use, spark, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# wenn du dataframe direkt packst entferne das und statt path einfach df als parameter\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_venue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns_to_use\u001b[49m:\n\u001b[0;32m     21\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mwhen(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_venue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmod\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvldb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     23\u001b[0m     transform_author_names_udf \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mudf(\u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(name\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m name\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39msplit()]), StringType())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py:1369\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert column into bool: please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when building DataFrame boolean expressions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 1.8916327953338623 seconds. Number of index combinations: 782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x11b3e8310>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example that other are functioning\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkk\").getOrCreate()\n",
    "\n",
    "acm_df = spark.read.csv(acm_csv, header=True, inferSchema=True)\n",
    "dblp_df = spark.read.csv(dblp_csv, header=True, inferSchema=True)\n",
    "\n",
    "selected_columns = ['author_names', 'paper_title']\n",
    "c = ngram.initial_ngram_parallel_df(spark,acm_df, selected_columns,2)\n",
    "d = ngram.initial_ngram_parallel_df(spark, dblp_df, selected_columns,2)\n",
    "e = match.apply_similarity_blocks_spark(c,d, 0.7, sim.jaccard_similarity)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
